import whisper
from openai import AzureOpenAI
import sys
import os
import requests



# MS_PROJECT_AINURI ë£¨íŠ¸ ê²½ë¡œ ë“±ë¡
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

# ì •í™•í•œ ê²½ë¡œë¡œ import
from Project_Backend.speechToText.speechDAO import SpeechDAO

dao = SpeechDAO()
#######################################################################################3

# ëª¨ë¸ ë¡œë”©
model = whisper.load_model("base")  # ì„±ëŠ¥ ë”°ë¼ ë‹¤ë¥¸ ëª¨ë¸ ì„ íƒ ê°€ëŠ¥

# ê²½ë¡œ í™•ì¸ (ì„ íƒ ì‚¬í•­)
TxtResult = model.transcribe("uploaded_audios/audioSample.m4a", language="ko")



# ì¶œë ¥
print("ğŸ“ ì¸ì‹ëœ í…ìŠ¤íŠ¸:")
print(TxtResult["text"])

#################################################################

client = AzureOpenAI(
    azure_endpoint="https://team01-05-4067-resource.openai.azure.com/", # í”„ë¡œì íŠ¸ ê°œìš”ìª½ ë¼ì´ë¸ŒëŸ¬ë¦¬ - Azure OpenAI ì—”ë“œí¬ì¸íŠ¸
   # í”„ë¡œì íŠ¸ ê°œìš”ìª½ API í‚¤
    api_version="2024-12-01-preview", 
)

conversation = [{
    "role": "system",
    "content": (
        "ë„ˆëŠ” ì‹ ê³  ë‚´ìš©ì„ ë‹¤ìŒ ì„¸ í•­ëª©ìœ¼ë¡œ ê°„ë‹¨íˆ ì •ë¦¬í•˜ëŠ” ë„ìš°ë¯¸ì•¼.\n"
        "1. ì¥ì†Œ\n2. ê³µê³µê¸°ë¬¼ ì¢…ë¥˜\n3. ë°œê²¬ëœ ë¬¸ì œ ë˜ëŠ” ì ê²€ í•„ìš” ì‚¬ìœ \n"
        "ê° í•­ëª©ì€ í•œ ì¤„ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ê³ , ë¶ˆí•„ìš”í•œ ë¬¸ì¥ì€ ì¶”ê°€í•˜ì§€ ë§ˆ."
    )
}]

userInput = TxtResult["text"]
conversation.append({"role": "user", "content": userInput})
res = client.chat.completions.create(
        model="gpt-4.1", # ????
        messages=conversation,
        extra_body={
            "data_sources": [
                {
                    "type": "azure_search",
                    "parameters": {
                        "endpoint": "https://ainuri-search.search.windows.net", # ai search ê°œìš”ìª½ì— URL
                        "index_name": "ainuri-index", # ì•„ê¹Œ ì €ì¥í–ˆë˜ ì´ë¦„
                        "authentication": {
                            # ë”°ë¡œ ai searchë¦¬ì†ŒìŠ¤ ì°¾ì•„ ë“¤ì–´ê°€ì„œ ì„¤ì • - í‚¤ - ê¸°ë³¸ ê´€ë¦¬ì í‚¤
                        },
                        "embedding_dependency": {
                            "type": "endpoint",
                            "endpoint": "https://team01-05-4067-resource.cognitiveservices.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview", # ë‚´ ìì‚° - ëª¨ë¸ + ì—”ë“œí¬ì¸íŠ¸ìª½ ëŒ€ìƒ URI
                            "authentication": {
                                 # ë‚´ ìì‚° - ëª¨ë¸ + ì—”ë“œí¬ì¸íŠ¸ìª½ í‚¤
                            },
                        },
                        
                    },
                }
            ]
        },
    )
reportResult = res.choices[0].message.content
parsed = dao.parse_report_result(reportResult)
print("---------------------")
print("gpt êµ¬ì¡°í™” ")
print(reportResult)
print("---------------------")
print(parsed)



# â‘¢ FastAPIë¡œ GET ìš”ì²­ ë³´ë‚´ê¸°
params = {
    "location": parsed["ì¥ì†Œ"],
    "type": parsed["ê³µê³µê¸°ë¬¼ ì¢…ë¥˜"],
    "problem": parsed["ë°œê²¬ëœ ë¬¸ì œ ë˜ëŠ” ì ê²€ í•„ìš” ì‚¬ìœ "]
}

response = requests.get("http://localhost:9999/info.speechToText", params=params)

# â‘£ ì‘ë‹µ í™•ì¸
print("---------------------")
print("FastAPI ì‘ë‹µ ê²°ê³¼:")
print(response.json())